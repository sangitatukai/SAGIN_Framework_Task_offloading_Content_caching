# rl_baseline_comparison.py - FIXED Compare RL agents against baseline approaches
import numpy as np
import matplotlib.pyplot as plt
import time
import pandas as pd
import torch  # ADD THIS LINE - was missing!
import torch.nn.functional as F  # ADD THIS LINE too
from typing import Dict, List, Tuple
from collections import defaultdict

# Import baseline system and RL system
from main_baseline_comparison import run_baseline_comparison
from rl_formulation_sagin import HierarchicalSAGINAgent
from sagin_env import SAGINEnv, SystemDownException


class ComprehensiveComparison:
    """
    Comprehensive comparison system between baseline approaches and RL agents
    """

    def __init__(self, grid_size: Tuple[int, int] = (3, 3), num_trials: int = 2):
        self.grid_size = grid_size
        self.num_trials = num_trials
        self.comparison_results = {}

        # System parameters
        self.system_params = {
            'X': grid_size[0], 'Y': grid_size[1], 'duration': 300,
            'cache_size': 40, 'compute_power_uav': 25, 'compute_power_sat': 200,
            'energy': 80000, 'max_queue': 15, 'num_sats': 2,
            'num_iot_per_region': 20, 'max_active_iot': 10, 'ofdm_slots': 6
        }

    def run_baseline_approaches(self) -> Dict[str, Dict]:
        """Run all baseline approach combinations"""
        print("üì¨ Running Baseline Approaches")
        print("=" * 50)

        # Define baseline configurations
        baseline_configs = [
            {
                'name': 'Greedy-All',
                'aggregation': 'greedy',
                'caching': 'greedy',
                'offloading': 'content_aware',
                'ofdm': 'priority_based'
            },
            {
                'name': 'Random-Split',
                'aggregation': 'random',
                'caching': 'greedy',
                'offloading': 'random_split',
                'ofdm': 'random'
            },
            {
                'name': 'Load-Balanced',
                'aggregation': 'greedy',
                'caching': 'greedy',
                'offloading': 'load_balanced',
                'ofdm': 'round_robin'
            },
            {
                'name': 'Fairness-Aware',
                'aggregation': 'greedy',
                'caching': 'greedy',
                'offloading': 'content_aware',
                'ofdm': 'fairness_aware'
            }
        ]

        baseline_results = {}

        for config in baseline_configs:
            config_name = config['name']
            print(f"\nüìä Testing {config_name}...")

            trial_results = []

            for trial in range(self.num_trials):
                print(f"   Trial {trial + 1}/{self.num_trials}")

                try:
                    # Create fresh environment
                    env = SAGINEnv(**self.system_params)

                    # Run baseline simulation with TIMESTEP LIMIT
                    episode_results = []
                    MAX_TIMESTEPS = 25  # Same limit for all approaches

                    for timestep in range(MAX_TIMESTEPS):
                        try:
                            # Execute baseline step
                            env.step()

                            # Collect performance
                            performance = env.get_performance_summary()
                            episode_results.append(performance)

                            # Early termination check
                            min_energy = min(uav.energy for uav in env.uavs.values())
                            if min_energy <= 1000:  # Energy threshold
                                print(f"      Early termination at timestep {timestep + 1}")
                                break

                        except SystemDownException:
                            print(f"      System down at timestep {timestep + 1}")
                            break
                        except Exception as e:
                            print(f"      Error at timestep {timestep + 1}: {e}")
                            break

                    if episode_results:
                        # Calculate trial metrics
                        final_performance = episode_results[-1]
                        avg_metrics = {
                            'success_rate': np.mean([p['overall_success_rate'] for p in episode_results]),
                            'cache_hit_rate': np.mean([p['cache_hit_rate'] for p in episode_results]),
                            'energy_efficiency': np.mean([p['energy_efficiency'] for p in episode_results]),
                            'dropped_tasks': final_performance['dropped_tasks'],
                            'timesteps_completed': len(episode_results)
                        }
                        trial_results.append(avg_metrics)
                        print(
                            f"      Success: {avg_metrics['success_rate']:.1f}%, {avg_metrics['timesteps_completed']} timesteps")

                except Exception as e:
                    print(f"   Trial {trial + 1} terminated: {e}")

            if trial_results:
                # Aggregate trial results
                baseline_results[config_name] = {
                    'mean_success_rate': np.mean([r['success_rate'] for r in trial_results]),
                    'std_success_rate': np.std([r['success_rate'] for r in trial_results]),
                    'mean_cache_hit_rate': np.mean([r['cache_hit_rate'] for r in trial_results]),
                    'std_cache_hit_rate': np.std([r['cache_hit_rate'] for r in trial_results]),
                    'mean_energy_efficiency': np.mean([r['energy_efficiency'] for r in trial_results]),
                    'std_energy_efficiency': np.std([r['energy_efficiency'] for r in trial_results]),
                    'mean_dropped_tasks': np.mean([r['dropped_tasks'] for r in trial_results]),
                    'std_dropped_tasks': np.std([r['dropped_tasks'] for r in trial_results]),
                    'mean_timesteps': np.mean([r['timesteps_completed'] for r in trial_results]),
                    'config': config,
                    'num_trials': len(trial_results)
                }

                print(
                    f"   ‚úÖ {config_name}: Success={baseline_results[config_name]['mean_success_rate']:.1f}%¬±{baseline_results[config_name]['std_success_rate']:.1f}")

        return baseline_results

    def calculate_nuclear_reward(self, env, previous_env=None, timestep=0, episode=0):
        """üöÄ ENHANCED NUCLEAR REWARD - Ultra-dense feedback for aggressive learning"""

        current_perf = env.get_performance_summary()

        # üéØ PRIMARY OBJECTIVES (MASSIVE SCALING)
        success_rate = current_perf['overall_success_rate']
        cache_hit_rate = current_perf['cache_hit_rate']
        energy_efficiency = current_perf['energy_efficiency']
        dropped_tasks = current_perf['dropped_tasks']

        # üöÄ TURBOCHARGED BASE REWARDS (15x LARGER!)
        success_reward = success_rate * 300.0  # 0-30,000 points
        cache_reward = cache_hit_rate * 150.0  # 0-15,000 points
        efficiency_reward = energy_efficiency * 75000  # 0-75 points

        # üí• IMMEDIATE TASK COMPLETION REWARDS (NEW!)
        immediate_task_bonus = 0
        immediate_cache_bonus = 0
        try:
            for uav in env.uavs.values():
                # Reward each completed task this timestep
                completed_tasks = getattr(uav, 'completed_tasks_this_step', 0)
                immediate_task_bonus += completed_tasks * 100.0  # 100 points per task!

                # Reward each cache hit this timestep
                cache_hits = getattr(uav, 'cache_hits_this_step', 0)
                immediate_cache_bonus += cache_hits * 50.0  # 50 points per hit!

                # Energy conservation bonus
                energy_ratio = uav.energy / getattr(uav, 'max_energy', 80000)
                if energy_ratio > 0.8:
                    immediate_task_bonus += 25.0  # Well-managed energy
        except:
            pass

        # üî• MASSIVE IMPROVEMENT BONUSES
        improvement_bonus = 0
        if previous_env is not None:
            try:
                prev_perf = previous_env.get_performance_summary()
                success_improvement = success_rate - prev_perf['overall_success_rate']
                cache_improvement = cache_hit_rate - prev_perf['cache_hit_rate']

                # MASSIVE bonuses for ANY improvement
                improvement_bonus = (
                        success_improvement * 1000.0 +  # 1000 points per 1% improvement!
                        cache_improvement * 500.0  # 500 points per 1% cache improvement
                )

                # HUGE streak bonuses
                if success_improvement > 0:
                    improvement_bonus += 2000.0  # Success streak bonus
                if cache_improvement > 0:
                    improvement_bonus += 1000.0  # Cache improvement streak

            except:
                improvement_bonus = 0

        # üéØ BASELINE BEATING BONUS (CRITICAL!)
        baseline_beating_bonus = 0
        target_baseline = 39.0  # Beat the Load-Balanced baseline
        if success_rate > target_baseline:
            baseline_beating_bonus = (success_rate - target_baseline) * 2000.0  # 2000 pts per % above!

        # üèÜ MILESTONE BONUSES
        milestone_bonus = 0
        if success_rate > 40:
            milestone_bonus += 5000.0  # 40% milestone
        if success_rate > 42:
            milestone_bonus += 7500.0  # 42% milestone
        if success_rate > 45:
            milestone_bonus += 10000.0  # 45% milestone
        if success_rate > 48:
            milestone_bonus += 15000.0  # 48% milestone

        # üî• COORDINATION REWARDS (NEW!)
        coordination_bonus = 0
        try:
            uav_loads = [len(getattr(uav, 'task_queue', [])) for uav in env.uavs.values()]
            if len(uav_loads) > 1:
                avg_load = np.mean(uav_loads)
                load_balance_score = 1.0 / (1.0 + np.std(uav_loads))  # Higher = more balanced
                coordination_bonus = load_balance_score * 500.0  # Reward load balancing
        except:
            pass

        # ‚ö° EARLY TRAINING EXPLORATION BONUS
        exploration_bonus = 0
        if episode < 200:  # First 200 episodes
            exploration_bonus = (200 - episode) * 5.0  # Decreasing exploration bonus

        # üî• PENALTIES (Aggressive but fair)
        drop_penalty = dropped_tasks * 50.0  # 10x larger penalty
        energy_penalty = 0

        try:
            for uav in env.uavs.values():
                energy_ratio = uav.energy / getattr(uav, 'max_energy', 80000)
                if energy_ratio < 0.05:  # Critical energy
                    energy_penalty += (0.05 - energy_ratio) * 10000  # MASSIVE penalty
                elif energy_ratio < 0.2:  # Low energy
                    energy_penalty += (0.2 - energy_ratio) * 2000  # Large penalty
        except:
            pass

        # üìä FINAL REWARD CALCULATION
        total_reward = (
                success_reward + cache_reward + efficiency_reward +
                immediate_task_bonus + immediate_cache_bonus +
                improvement_bonus + baseline_beating_bonus + milestone_bonus +
                coordination_bonus + exploration_bonus -
                drop_penalty - energy_penalty
        )

        # Episode scaling factor (more aggressive rewards in later episodes)
        if episode > 100:
            episode_scaling = 1.0 + (episode - 100) / 1000.0  # Up to 40% boost
            total_reward *= episode_scaling

        # Ensure reasonable bounds but allow large positive rewards
        total_reward = max(-20000, min(total_reward, 100000))

        # Ensure minimum positive reward for learning
        if total_reward < 100.0:
            total_reward = 100.0 + np.random.random() * 50.0

        # Debug logging every 100 episodes
        if timestep == 0 and episode % 100 == 0:
            print(f"      üíé Episode {episode} Enhanced Reward Breakdown:")
            print(f"         Success: {success_reward:.0f}, Cache: {cache_reward:.0f}")
            print(f"         Immediate: {immediate_task_bonus + immediate_cache_bonus:.0f}")
            print(f"         Baseline Bonus: {baseline_beating_bonus:.0f}")
            print(f"         Milestones: {milestone_bonus:.0f}")
            print(f"         Total: {total_reward:.0f}")

        return total_reward

    def calculate_dense_learning_reward(self, env, previous_env=None, timestep=0):
        """Dense reward function for better RL learning"""
        import numpy as np

        current_perf = env.get_performance_summary()

        # Primary objectives
        success_rate = current_perf['overall_success_rate']
        cache_hit_rate = current_perf['cache_hit_rate']
        energy_efficiency = current_perf['energy_efficiency']
        dropped_tasks = current_perf['dropped_tasks']

        # Base rewards
        success_reward = success_rate * 5.0
        cache_reward = cache_hit_rate * 3.0
        energy_reward = energy_efficiency * 200000

        # Improvement bonus
        improvement_bonus = 0
        if previous_env is not None:
            try:
                prev_perf = previous_env.get_performance_summary()
                success_improvement = success_rate - prev_perf['overall_success_rate']
                cache_improvement = cache_hit_rate - prev_perf['cache_hit_rate']
                improvement_bonus = success_improvement * 20.0 + cache_improvement * 10.0
            except:
                improvement_bonus = 0

        # Penalties
        drop_penalty = dropped_tasks * 1.0
        energy_penalty = 0

        try:
            for uav in env.uavs.values():
                energy_ratio = uav.energy / 80000
                if energy_ratio < 0.2:
                    energy_penalty += (0.2 - energy_ratio) * 100
        except:
            energy_penalty = 0

        # Exploration bonus for early training
        exploration_bonus = 5.0 if timestep < 1000 else 0.0

        # Final reward
        total_reward = (
                success_reward + cache_reward + energy_reward +
                improvement_bonus + exploration_bonus -
                drop_penalty - energy_penalty
        )

        # Ensure reasonable bounds
        total_reward = max(-1000, min(total_reward, 2000))
        return max(total_reward, 1.0)

    # ================================================================
    # REPLACE YOUR run_rl_approaches() METHOD WITH THIS NUCLEAR VERSION
    # ================================================================

    def run_rl_approaches(self) -> Dict[str, Dict]:
        """üî• NUCLEAR RL TRAINING - BEAST MODE"""
        print("\nüî• NUCLEAR RL TRAINING - BEAST MODE ACTIVATED")
        print("=" * 60)

        rl_configs = [
            {
                'name': 'Nuclear-SAGIN-Agent',
                'episodes': 2000,  # 3x more training!
                'description': 'Nuclear-powered RL agent'
            }
        ]
        rl_results = {}

        for config in rl_configs:
            config_name = config['name']
            num_episodes = config['episodes']
            print(f"\nüöÄ Unleashing {config_name} ({num_episodes} episodes)...")

            trial_results = []

            for trial in range(self.num_trials):
                print(f"   üî• Trial {trial + 1}/{self.num_trials}")

                try:
                    # Import nuclear agent
                    from rl_formulation_sagin import NuclearSAGINAgent
                    rl_agent = NuclearSAGINAgent(
                        grid_size=self.grid_size,
                        learning_rate=5e-4  # Increased from 1e-3 for better learning
                    )
                    print(f"   üß† Nuclear Agent: {sum(p.numel() for p in rl_agent.parameters()):,} parameters")

                    # NUCLEAR TRAINING PHASE
                    episode_performances = []
                    training_rewards = []
                    self.episode_performances = episode_performances  # For reward calculation

                    for episode in range(num_episodes):
                        try:
                            # Fresh environment
                            env = SAGINEnv(**self.system_params)
                            rl_agent.reset_temporal_states()

                            episode_reward = 0.0
                            previous_env = None
                            MAX_TIMESTEPS = 25

                            # üéØ EPISODE EXECUTION
                            for timestep in range(MAX_TIMESTEPS):
                                try:
                                    # Extract states
                                    uav_states = self._extract_uav_states(env)
                                    active_devices = self._extract_active_devices(env)
                                    device_contents = self._extract_device_contents(env)
                                    task_bursts = self._extract_task_bursts(env)
                                    candidate_content = self._extract_candidate_content(env)
                                    uav_positions = self._extract_uav_positions(env)

                                    if not uav_states:
                                        break

                                    # Store current state
                                    current_state = {
                                        'uav_states': uav_states,
                                        'global_context': rl_agent._get_system_context(uav_states)
                                    }

                                    # üß† NUCLEAR RL DECISIONS
                                    selected_devices = rl_agent.step_iot_aggregation(
                                        uav_states, active_devices, device_contents)
                                    slot_allocation = rl_agent.step_ofdm_allocation(uav_states, max_slots=6)
                                    offloading_decisions, caching_decisions = rl_agent.step_caching_offloading(
                                        uav_states, task_bursts, candidate_content, uav_positions)

                                    # Store actions
                                    actions = {
                                        'iot': selected_devices,
                                        'ofdm': slot_allocation,
                                        'offloading': offloading_decisions,
                                        'caching': caching_decisions
                                    }

                                    # Step environment
                                    env.step()

                                    # üî• NUCLEAR REWARD CALCULATION
                                    step_reward = self.calculate_nuclear_reward(
                                        env, previous_env, timestep, episode
                                    )
                                    episode_reward += step_reward

                                    # üìä COLLECT EXPERIENCE FOR LEARNING
                                    next_state = {'uav_states': self._extract_uav_states(env)}
                                    rl_agent.collect_experience(
                                        current_state, actions, step_reward, next_state, False
                                    )

                                    previous_env = env

                                    # Early termination
                                    min_energy = min(uav.energy for uav in env.uavs.values())
                                    if min_energy <= 1000:
                                        rl_agent.collect_experience(
                                            current_state, actions, step_reward, next_state, True
                                        )
                                        break

                                except Exception:
                                    break

                            # üöÄ CRITICAL: POLICY UPDATES (ACTUAL LEARNING)
                            if episode % 2 == 0 and episode > 0:
                                try:
                                    losses = rl_agent.update_policies()
                                    if episode % 100 == 0:
                                        print(f"      üß† Episode {episode}: Loss={losses['total_loss']:.4f}, "
                                              f"Explore={rl_agent.exploration_rate:.3f}")
                                except:
                                    pass

                            # Record episode performance
                            training_rewards.append(episode_reward)
                            try:
                                final_performance = env.get_performance_summary()
                                episode_performances.append(final_performance['overall_success_rate'])
                            except:
                                episode_performances.append(0.0)

                            # üìä PROGRESS REPORTING
                            if (episode + 1) % 100 == 0:
                                recent_reward = np.mean(training_rewards[-100:])
                                recent_success = np.mean(episode_performances[-100:])

                                if episode >= 200:
                                    early_success = np.mean(episode_performances[:100])
                                    improvement = recent_success - early_success
                                    print(f"      üöÄ Episode {episode + 1}: Reward={recent_reward:.1f}, "
                                          f"Success={recent_success:.1f}% (+{improvement:.1f}%)")
                                else:
                                    print(f"      üöÄ Episode {episode + 1}: Reward={recent_reward:.1f}, "
                                          f"Success={recent_success:.1f}%")

                            # Learning rate decay
                            if episode % 200 == 0 and episode > 0:
                                for optimizer in rl_agent.optimizers.values():
                                    for param_group in optimizer.param_groups:
                                        param_group['lr'] *= 0.9

                        except Exception:
                            episode_performances.append(0.0)
                            training_rewards.append(0.0)

                    print(f"   üî• Nuclear training completed!")

                    # üìä TRAINING ANALYSIS
                    if len(episode_performances) >= 200:
                        early_perf = np.mean(episode_performances[:100])
                        late_perf = np.mean(episode_performances[-100:])
                        improvement = late_perf - early_perf

                        print(f"   üìà Training Analysis:")
                        print(f"      Early: {early_perf:.1f}% ‚Üí Final: {late_perf:.1f}%")
                        print(f"      üöÄ IMPROVEMENT: {improvement:.1f}%")

                        if improvement > 15:
                            print("   üèÜ NUCLEAR SUCCESS: Massive learning!")
                        elif improvement > 10:
                            print("   üî• EXCELLENT: Strong learning!")
                        elif improvement > 5:
                            print("   ‚úÖ GOOD: Solid progress!")
                    else:
                        improvement = 0.0

                    # üéØ NUCLEAR EVALUATION
                    print(f"   üî• Nuclear evaluation...")
                    eval_episodes = []

                    for eval_ep in range(10):
                        try:
                            env = SAGINEnv(**self.system_params)
                            rl_agent.reset_temporal_states()
                            eval_episode_performances = []

                            for timestep in range(20):
                                try:
                                    uav_states = self._extract_uav_states(env)
                                    active_devices = self._extract_active_devices(env)
                                    device_contents = self._extract_device_contents(env)
                                    task_bursts = self._extract_task_bursts(env)
                                    candidate_content = self._extract_candidate_content(env)
                                    uav_positions = self._extract_uav_positions(env)

                                    if not uav_states:
                                        break

                                    # Deterministic evaluation (no exploration)
                                    with torch.no_grad():
                                        old_exploration = rl_agent.exploration_rate
                                        rl_agent.exploration_rate = 0.0

                                        selected_devices = rl_agent.step_iot_aggregation(
                                            uav_states, active_devices, device_contents)
                                        slot_allocation = rl_agent.step_ofdm_allocation(uav_states, max_slots=6)
                                        offloading_decisions, caching_decisions = rl_agent.step_caching_offloading(
                                            uav_states, task_bursts, candidate_content, uav_positions)

                                        rl_agent.exploration_rate = old_exploration

                                    env.step()
                                    performance = env.get_performance_summary()
                                    eval_episode_performances.append(performance)

                                    min_energy = min(uav.energy for uav in env.uavs.values())
                                    if min_energy <= 1000:
                                        break

                                except:
                                    break

                            if eval_episode_performances:
                                final_perf = eval_episode_performances[-1]
                                eval_episodes.append({
                                    'success_rate': np.mean(
                                        [p['overall_success_rate'] for p in eval_episode_performances]),
                                    'cache_hit_rate': np.mean([p['cache_hit_rate'] for p in eval_episode_performances]),
                                    'energy_efficiency': np.mean(
                                        [p['energy_efficiency'] for p in eval_episode_performances]),
                                    'dropped_tasks': final_perf['dropped_tasks'],
                                    'timesteps_completed': len(eval_episode_performances)
                                })
                                print(f"      üéØ Eval {eval_ep + 1}: Success={eval_episodes[-1]['success_rate']:.1f}%")

                        except Exception:
                            pass

                    # Create trial result
                    if eval_episodes:
                        trial_result = {
                            'success_rate': np.mean([e['success_rate'] for e in eval_episodes]),
                            'cache_hit_rate': np.mean([e['cache_hit_rate'] for e in eval_episodes]),
                            'energy_efficiency': np.mean([e['energy_efficiency'] for e in eval_episodes]),
                            'dropped_tasks': np.mean([e['dropped_tasks'] for e in eval_episodes]),
                            'timesteps_completed': np.mean([e['timesteps_completed'] for e in eval_episodes]),
                            'training_improvement': improvement,
                            'final_reward': np.mean(training_rewards[-100:]) if training_rewards else 0.0
                        }
                        trial_results.append(trial_result)
                        print(f"   üî• Trial {trial + 1}: Success={trial_result['success_rate']:.1f}%, "
                              f"Improvement={trial_result['training_improvement']:.1f}%")
                    else:
                        print(f"   üí• Trial {trial + 1} evaluation failed")

                except Exception as e:
                    print(f"   üí• Trial {trial + 1} failed: {e}")

            # Aggregate results
            if trial_results:
                rl_results[config_name] = {
                    'mean_success_rate': np.mean([r['success_rate'] for r in trial_results]),
                    'std_success_rate': np.std([r['success_rate'] for r in trial_results]),
                    'mean_cache_hit_rate': np.mean([r['cache_hit_rate'] for r in trial_results]),
                    'std_cache_hit_rate': np.std([r['cache_hit_rate'] for r in trial_results]),
                    'mean_energy_efficiency': np.mean([r['energy_efficiency'] for r in trial_results]),
                    'std_energy_efficiency': np.std([r['energy_efficiency'] for r in trial_results]),
                    'mean_dropped_tasks': np.mean([r['dropped_tasks'] for r in trial_results]),
                    'std_dropped_tasks': np.std([r['dropped_tasks'] for r in trial_results]),
                    'mean_timesteps': np.mean([r['timesteps_completed'] for r in trial_results]),
                    'mean_training_improvement': np.mean([r['training_improvement'] for r in trial_results]),
                    'mean_final_reward': np.mean([r['final_reward'] for r in trial_results]),
                    'config': config,
                    'num_trials': len(trial_results)
                }

                result = rl_results[config_name]
                print(f"\nüî• {config_name} NUCLEAR RESULTS:")
                print(f"   üéØ Success Rate: {result['mean_success_rate']:.1f}%¬±{result['std_success_rate']:.1f}")
                print(f"   üöÄ Training Improvement: {result['mean_training_improvement']:.1f}%")
                print(f"   üíé Final Reward: {result['mean_final_reward']:.1f}")

                # üèÜ PERFORMANCE ASSESSMENT
                if result['mean_success_rate'] > 45:
                    print("   üèÜ NUCLEAR SUCCESS: DESTROYED THE BASELINES! üî•")
                elif result['mean_success_rate'] > 40:
                    print("   üöÄ EXCELLENT: Competitive with baselines!")
                elif result['mean_success_rate'] > 35:
                    print("   ‚úÖ GOOD: Solid performance!")
                elif result['mean_training_improvement'] > 10:
                    print("   üìà PROMISING: Strong learning detected!")
                else:
                    print("   ‚ö†Ô∏è  NEEDS WORK: Check implementation")

            else:
                print(f"   üí• {config_name}: All trials failed")

        return rl_results

    # ========================================
    # STEP 2: Add these helper methods to your ComprehensiveComparison class
    # ========================================

    def calculate_improved_reward(self, performance, previous_performance, env):
        """Enhanced reward function designed for better RL learning"""

        # Primary objectives (properly scaled)
        success_rate = performance['overall_success_rate']  # 0-100
        cache_hit_rate = performance['cache_hit_rate']  # 0-100
        energy_efficiency = performance['energy_efficiency'] * 1000  # Scale to 0-1 range

        # Base reward components
        success_reward = success_rate * 0.5  # 0-50 points
        cache_reward = cache_hit_rate * 0.2  # 0-20 points
        efficiency_reward = energy_efficiency  # 0-1 points

        # Penalty components
        drop_penalty = performance['dropped_tasks'] * 0.01  # Small penalty per dropped task

        # Energy penalty for low UAV energy (encourages energy conservation)
        energy_penalty = 0
        low_energy_count = 0
        for uav in env.uavs.values():
            energy_ratio = uav.energy / 80000  # Normalize to initial energy
            if energy_ratio < 0.3:  # Below 30% energy
                energy_penalty += (0.3 - energy_ratio) * 10
                low_energy_count += 1

        # Bonus for improvement over previous timestep
        improvement_bonus = 0
        if previous_performance is not None:
            success_improvement = success_rate - previous_performance['overall_success_rate']
            cache_improvement = cache_hit_rate - previous_performance['cache_hit_rate']
            improvement_bonus = (success_improvement * 0.5 + cache_improvement * 0.2)

        # Diversity bonus (encourage exploring different actions)
        diversity_bonus = 0.1  # Small constant bonus for taking actions

        # Calculate final reward
        total_reward = (
                success_reward +
                cache_reward +
                efficiency_reward +
                improvement_bonus +
                diversity_bonus -
                drop_penalty -
                energy_penalty
        )

        # Ensure reward is not negative (helps with learning stability)
        total_reward = max(total_reward, 0.1)

        return total_reward

    def get_curriculum_params(self, episode, total_episodes):
        """Curriculum learning: start easy, gradually increase difficulty"""

        progress = episode / total_episodes

        if progress < 0.3:  # First 30% - Easy
            return {
                'max_active_iot': 8,  # Fewer devices
                'energy': 100000,  # More energy
                'num_iot_per_region': 15  # Fewer IoT devices total
            }
        elif progress < 0.6:  # Next 30% - Medium
            return {
                'max_active_iot': 10,
                'energy': 90000,
                'num_iot_per_region': 18
            }
        else:  # Final 40% - Full difficulty
            return {
                'max_active_iot': 12,
                'energy': 80000,
                'num_iot_per_region': 20
            }

    def add_exploration_noise(self, actions, episode, total_episodes):
        """Add exploration noise to actions during training"""

        exploration_rate = max(0.05, 1.0 - (episode / (total_episodes * 0.7)))

        if isinstance(actions, dict):
            for key, action in actions.items():
                if isinstance(action, torch.Tensor) and torch.rand(1).item() < exploration_rate:
                    noise_scale = 0.1 * exploration_rate
                    noise = torch.randn_like(action) * noise_scale
                    actions[key] = torch.clamp(action + noise, 0, action.max() + 1)

        return actions

    def update_rl_policies(self, rl_agent, experiences):
        """Update RL agent policies using collected experiences"""

        if len(experiences) < 32:
            return {'total_loss': 0.0}

        # Simple policy update (you may need to adapt this to your specific RL agent implementation)
        total_loss = 0.0

        try:
            # Calculate returns for the experiences
            returns = []
            running_return = 0
            for exp in reversed(experiences):
                running_return = exp['reward'] + 0.99 * running_return * (1 - exp['done'])
                returns.insert(0, running_return)

            returns = torch.tensor(returns, dtype=torch.float32)

            # Normalize returns
            if len(returns) > 1:
                returns = (returns - returns.mean()) / (returns.std() + 1e-8)

            # Update policies (simplified - adapt to your agent's architecture)
            if hasattr(rl_agent, 'update_policies'):
                loss = rl_agent.update_policies(experiences, returns)
                total_loss = loss
            else:
                # Fallback: just ensure gradients flow through the network
                dummy_loss = torch.tensor(0.0, requires_grad=True)
                for param in rl_agent.parameters():
                    dummy_loss = dummy_loss + param.mean() * 1e-6

                if hasattr(rl_agent, 'optimizer'):
                    rl_agent.optimizer.zero_grad()
                    dummy_loss.backward()
                    rl_agent.optimizer.step()

                total_loss = dummy_loss.item()

        except Exception as e:
            print(f"      Policy update failed: {e}")
            total_loss = 0.0

        return {'total_loss': total_loss}

    def adjust_learning_rate(self, rl_agent, episode, total_episodes):
        """Adjust learning rate during training"""

        progress = episode / total_episodes

        # Learning rate schedule
        if progress < 0.5:
            lr_factor = 1.0
        elif progress < 0.8:
            lr_factor = 0.5
        else:
            lr_factor = 0.1

        # Apply to all optimizers
        base_lr = 3e-4
        new_lr = base_lr * lr_factor

        optimizers = []
        if hasattr(rl_agent, 'optimizer'):
            optimizers.append(rl_agent.optimizer)

        # Check for component-specific optimizers
        for attr_name in dir(rl_agent):
            if 'optimizer' in attr_name.lower() and attr_name != 'optimizer':
                attr = getattr(rl_agent, attr_name)
                if hasattr(attr, 'param_groups'):
                    optimizers.append(attr)

        for optimizer in optimizers:
            for param_group in optimizer.param_groups:
                param_group['lr'] = new_lr

    def debug_rl_training(self, rl_agent, episode, episode_reward, success_rate):
        """Enhanced debugging for RL training"""

        print(f"\nüîç DEBUG Episode {episode}:")
        print(f"   Reward: {episode_reward:.3f}")
        print(f"   Success Rate: {success_rate:.1f}%")

        # Check gradient flow
        total_grad_norm = 0
        grad_count = 0
        for param in rl_agent.parameters():
            if param.grad is not None:
                total_grad_norm += param.grad.data.norm(2).item()
                grad_count += 1

        avg_grad_norm = total_grad_norm / max(grad_count, 1)
        print(f"   Avg Gradient Norm: {avg_grad_norm:.6f}")

        if avg_grad_norm < 1e-6:
            print("   üö® CRITICAL: Gradients too small - learning may have stopped!")
        elif avg_grad_norm > 10:
            print("   ‚ö†Ô∏è  WARNING: Gradients very large - may need gradient clipping!")

        # Check parameter updates
        param_count = sum(1 for _ in rl_agent.parameters())
        print(f"   Total Parameters: {param_count}")

        # Memory check
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024 ** 2  # MB
            print(f"   GPU Memory: {memory_used:.1f}MB")

    def check_rl_agent_implementation(self, rl_agent):
        """Comprehensive check of RL agent implementation"""

        print("   üîç Checking RL Agent Implementation...")
        issues = []

        # Check trainable parameters
        total_params = sum(p.numel() for p in rl_agent.parameters())
        trainable_params = sum(p.numel() for p in rl_agent.parameters() if p.requires_grad)

        print(f"   üìä Parameters: {total_params:,} total, {trainable_params:,} trainable")

        if trainable_params == 0:
            issues.append("No trainable parameters found!")

        # Check required methods
        required_methods = ['step_iot_aggregation', 'step_caching_offloading', 'step_ofdm_allocation']
        for method in required_methods:
            if not hasattr(rl_agent, method):
                issues.append(f"Missing method: {method}")

        # Test forward pass
        try:
            dummy_states = {(0, 0): {'zipf_param': 1.5, 'cache_hit_rate': 0.5, 'energy_ratio': 0.8, 'queue_ratio': 0.1}}
            dummy_devices = {(0, 0): []}
            dummy_contents = {(0, 0): []}

            # Test each component
            devices = rl_agent.step_iot_aggregation(dummy_states, dummy_devices, dummy_contents)
            ofdm = rl_agent.step_ofdm_allocation(dummy_states, max_slots=6)

            print("   ‚úÖ Forward pass test successful")

        except Exception as e:
            issues.append(f"Forward pass failed: {e}")

        # Report issues
        if issues:
            print("   üö® Implementation Issues Found:")
            for issue in issues:
                print(f"      ‚ùå {issue}")
            return False
        else:
            print("   ‚úÖ RL Agent implementation verified")
            return True

    def _extract_uav_states(self, env):
        """Extract UAV states for RL agent"""
        uav_states = {}
        for (x, y), uav in env.uavs.items():
            energy_ratio = uav.energy / uav.max_energy if uav.max_energy > 0 else 0
            queue_ratio = len(uav.queue) / uav.max_queue if uav.max_queue > 0 else 0
            cache_hit_rate = uav.cache_hits / max(uav.total_tasks, 1)

            region = env.iot_regions.get((x, y))
            zipf_param = getattr(region, 'current_zipf_param', 1.5) if region else 1.5

            uav_states[(x, y)] = {
                'energy_ratio': energy_ratio,
                'queue_ratio': queue_ratio,
                'cache_hit_rate': cache_hit_rate,
                'zipf_param': zipf_param,
                'aggregated_data': sum(content.get('size', 0) for content in uav.aggregated_content.values())
            }
        return uav_states

    def _extract_active_devices(self, env):
        """Extract active devices for RL agent"""
        active_devices = {}
        for (x, y), region in env.iot_regions.items():
            try:
                devices = region.sample_active_devices()
                active_devices[(x, y)] = devices
            except:
                active_devices[(x, y)] = []
        return active_devices

    def _extract_device_contents(self, env):
        """Extract device contents for RL agent"""
        device_contents = {}
        for (x, y), region in env.iot_regions.items():
            try:
                active_devices = region.sample_active_devices()
                if active_devices:
                    content_list = region.generate_content(active_devices, env.g_timestep, (x, y))
                    region_contents = {}
                    for content in content_list:
                        device_id = content.get('device_id')
                        if device_id is not None:
                            region_contents[device_id] = content
                    device_contents[(x, y)] = region_contents
                else:
                    device_contents[(x, y)] = {}
            except:
                device_contents[(x, y)] = {}
        return device_contents

    def _extract_task_bursts(self, env):
        """Extract task bursts for RL agent"""
        task_bursts = {}
        for (x, y), uav in env.uavs.items():
            try:
                tasks = uav.generate_tasks(env.X, env.Y, env.g_timestep)
                task_bursts[(x, y)] = tasks
            except:
                task_bursts[(x, y)] = []
        return task_bursts

    def _extract_candidate_content(self, env):
        """Extract candidate content for RL agent"""
        candidate_content = {}
        for (x, y), uav in env.uavs.items():
            candidates = []

            # Add cache content
            for cid, content in uav.cache_storage.items():
                content_copy = content.copy()
                content_copy['origin'] = 1
                content_copy['usefulness'] = 0.5
                candidates.append(content_copy)

            # Add aggregated content
            for cid, content in uav.aggregated_content.items():
                content_copy = content.copy()
                content_copy['origin'] = 0
                content_copy['usefulness'] = 0.7
                candidates.append(content_copy)

            candidate_content[(x, y)] = candidates
        return candidate_content

    def _extract_uav_positions(self, env):
        """Extract UAV positions for RL agent"""
        return {coord: uav.uav_pos for coord, uav in env.uavs.items()}

    def run_comprehensive_comparison(self) -> Dict[str, Dict]:
        """Run comprehensive comparison of all approaches"""
        print("üöÄ COMPREHENSIVE SAGIN COMPARISON: BASELINES vs RL AGENTS")
        print("=" * 80)
        print(f"üìä Configuration: {self.grid_size[0]}x{self.grid_size[1]} grid, {self.num_trials} trials per approach")
        print(f"‚öôÔ∏è System: {self.system_params['cache_size']}MB cache, {self.system_params['energy']:,}J energy")

        start_time = time.time()

        # Run baseline approaches
        baseline_results = self.run_baseline_approaches()

        # Run RL approaches
        rl_results = self.run_rl_approaches()

        # Combine results
        all_results = {**baseline_results, **rl_results}

        total_time = time.time() - start_time

        print(f"\n‚è±Ô∏è Total comparison time: {total_time:.1f}s")

        # Store results
        self.comparison_results = {
            'baselines': baseline_results,
            'rl_agents': rl_results,
            'all_results': all_results,
            'system_params': self.system_params,
            'comparison_time': total_time
        }

        return self.comparison_results

    def print_comparison_summary(self):
        """Print comprehensive comparison summary"""
        if not self.comparison_results:
            print("‚ùå No comparison results available. Run comparison first.")
            return

        print("\n" + "=" * 80)
        print("üìä COMPREHENSIVE COMPARISON RESULTS")
        print("=" * 80)

        all_results = self.comparison_results['all_results']

        if not all_results:
            print("‚ùå No results to display")
            return

        # Create comparison table
        print(f"{'Approach':<25} {'Success Rate (%)':<20} {'Cache Hit (%)':<20} {'Energy Eff.':<15} {'Dropped':<10}")
        print("-" * 90)

        # Sort by success rate
        sorted_results = sorted(
            all_results.items(),
            key=lambda x: x[1]['mean_success_rate'],
            reverse=True
        )

        for name, results in sorted_results:
            success_str = f"{results['mean_success_rate']:.1f}¬±{results['std_success_rate']:.1f}"
            cache_str = f"{results['mean_cache_hit_rate']:.1f}¬±{results['std_cache_hit_rate']:.1f}"
            energy_str = f"{results['mean_energy_efficiency']:.4f}¬±{results['std_energy_efficiency']:.4f}"
            dropped_str = f"{results['mean_dropped_tasks']:.0f}¬±{results['std_dropped_tasks']:.0f}"

            # Mark if this is RL approach
            marker = "üß†" if name.startswith('Hierarchical') or name.startswith('GRU') else "üìä"

            print(f"{marker} {name:<23} {success_str:<20} {cache_str:<20} {energy_str:<15} {dropped_str:<10}")

        # Performance improvement analysis
        print("\n" + "=" * 80)
        print("üìà PERFORMANCE IMPROVEMENT ANALYSIS")
        print("=" * 80)

        # Compare RL vs best baseline
        rl_approaches = {k: v for k, v in all_results.items() if 'Hierarchical' in k or 'GRU' in k}
        baseline_approaches = {k: v for k, v in all_results.items() if k not in rl_approaches}

        if rl_approaches and baseline_approaches:
            best_rl = max(rl_approaches.items(), key=lambda x: x[1]['mean_success_rate'])
            best_baseline = max(baseline_approaches.items(), key=lambda x: x[1]['mean_success_rate'])

            print(f"üèÜ Best RL Approach: {best_rl[0]}")
            print(f"   Success Rate: {best_rl[1]['mean_success_rate']:.1f}%")
            print(f"   Cache Hit Rate: {best_rl[1]['mean_cache_hit_rate']:.1f}%")
            print(f"   Energy Efficiency: {best_rl[1]['mean_energy_efficiency']:.4f}")

            print(f"\nüìä Best Baseline Approach: {best_baseline[0]}")
            print(f"   Success Rate: {best_baseline[1]['mean_success_rate']:.1f}%")
            print(f"   Cache Hit Rate: {best_baseline[1]['mean_cache_hit_rate']:.1f}%")
            print(f"   Energy Efficiency: {best_baseline[1]['mean_energy_efficiency']:.4f}")

            # Calculate improvements
            success_improvement = ((best_rl[1]['mean_success_rate'] - best_baseline[1]['mean_success_rate'])
                                   / best_baseline[1]['mean_success_rate'] * 100)
            cache_improvement = ((best_rl[1]['mean_cache_hit_rate'] - best_baseline[1]['mean_cache_hit_rate'])
                                 / max(best_baseline[1]['mean_cache_hit_rate'], 0.1) * 100)
            energy_improvement = ((best_rl[1]['mean_energy_efficiency'] - best_baseline[1]['mean_energy_efficiency'])
                                  / max(best_baseline[1]['mean_energy_efficiency'], 1e-6) * 100)

            print(f"\nüöÄ RL IMPROVEMENTS:")
            print(f"   Success Rate: {success_improvement:+.1f}%")
            print(f"   Cache Hit Rate: {cache_improvement:+.1f}%")
            print(f"   Energy Efficiency: {energy_improvement:+.1f}%")

    def plot_comparison_results(self):
        """Generate comprehensive comparison plots"""
        if not self.comparison_results:
            print("‚ùå No results to plot. Run comparison first.")
            return

        all_results = self.comparison_results['all_results']

        if not all_results:
            print("‚ùå No data available for plotting")
            return

        # Prepare data for plotting
        approaches = list(all_results.keys())
        success_rates = [all_results[name]['mean_success_rate'] for name in approaches]
        success_stds = [all_results[name]['std_success_rate'] for name in approaches]
        cache_hits = [all_results[name]['mean_cache_hit_rate'] for name in approaches]
        cache_stds = [all_results[name]['std_cache_hit_rate'] for name in approaches]
        energy_effs = [all_results[name]['mean_energy_efficiency'] for name in approaches]
        energy_stds = [all_results[name]['std_energy_efficiency'] for name in approaches]
        dropped_tasks = [all_results[name]['mean_dropped_tasks'] for name in approaches]

        # Create colors: RL approaches in blue, baselines in other colors
        colors = []
        for name in approaches:
            if 'Hierarchical' in name or 'GRU' in name:
                colors.append('royalblue')
            elif 'Greedy' in name:
                colors.append('lightcoral')
            elif 'Random' in name:
                colors.append('lightgreen')
            elif 'Load' in name:
                colors.append('orange')
            else:
                colors.append('lightgray')

        # Create comprehensive plot
        fig = plt.figure(figsize=(18, 12))

        # Success Rate Comparison
        plt.subplot(2, 3, 1)
        bars = plt.bar(range(len(approaches)), success_rates, yerr=success_stds,
                       color=colors, alpha=0.8, capsize=5)
        plt.title('Success Rate Comparison', fontweight='bold', fontsize=14)
        plt.ylabel('Success Rate (%)')
        plt.xticks(range(len(approaches)), [name[:12] for name in approaches], rotation=45, ha='right')
        plt.grid(True, alpha=0.3)

        # Add value labels on bars
        for i, (rate, std) in enumerate(zip(success_rates, success_stds)):
            plt.text(i, rate + std + 1, f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')

        # Cache Hit Rate Comparison
        plt.subplot(2, 3, 2)
        plt.bar(range(len(approaches)), cache_hits, yerr=cache_stds,
                color=colors, alpha=0.8, capsize=5)
        plt.title('Cache Hit Rate Comparison', fontweight='bold', fontsize=14)
        plt.ylabel('Cache Hit Rate (%)')
        plt.xticks(range(len(approaches)), [name[:12] for name in approaches], rotation=45, ha='right')
        plt.grid(True, alpha=0.3)

        # Energy Efficiency Comparison
        plt.subplot(2, 3, 3)
        plt.bar(range(len(approaches)), energy_effs, yerr=energy_stds,
                color=colors, alpha=0.8, capsize=5)
        plt.title('Energy Efficiency Comparison', fontweight='bold', fontsize=14)
        plt.ylabel('Tasks per Joule')
        plt.xticks(range(len(approaches)), [name[:12] for name in approaches], rotation=45, ha='right')
        plt.grid(True, alpha=0.3)

        # Dropped Tasks Comparison
        plt.subplot(2, 3, 4)
        plt.bar(range(len(approaches)), dropped_tasks, color=colors, alpha=0.8)
        plt.title('Dropped Tasks Comparison', fontweight='bold', fontsize=14)
        plt.ylabel('Number of Dropped Tasks')
        plt.xticks(range(len(approaches)), [name[:12] for name in approaches], rotation=45, ha='right')
        plt.grid(True, alpha=0.3)

        # Performance Radar Chart
        plt.subplot(2, 3, 5, projection='polar')

        # Show top 3 approaches
        top_approaches_idx = sorted(range(len(success_rates)), key=lambda i: success_rates[i], reverse=True)[:3]

        metrics = ['Success\nRate', 'Cache\nHit', 'Energy\nEff.', 'Low\nDropped']
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))

        max_success = max(success_rates)
        max_cache = max(cache_hits)
        max_energy = max(energy_effs)

        for idx in top_approaches_idx:
            # Normalize values (0-1 scale)
            normalized_values = [
                success_rates[idx] / max_success,
                cache_hits[idx] / max_cache,
                energy_effs[idx] / max_energy,
                1 - (dropped_tasks[idx] / max(dropped_tasks)) if max(dropped_tasks) > 0 else 1
            ]
            normalized_values.append(normalized_values[0])

            plt.plot(angles, normalized_values, 'o-', linewidth=2,
                     label=approaches[idx][:15] + ('...' if len(approaches[idx]) > 15 else ''))
            plt.fill(angles, normalized_values, alpha=0.1)

        plt.xticks(angles[:-1], metrics)
        plt.ylim(0, 1)
        plt.title('Performance Comparison\n(Normalized)', fontweight='bold', pad=20)
        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))

        # Statistical significance test visualization
        plt.subplot(2, 3, 6)

        # Group approaches by type
        rl_names = [name for name in approaches if 'Hierarchical' in name or 'GRU' in name]
        baseline_names = [name for name in approaches if name not in rl_names]

        if rl_names and baseline_names:
            rl_success = [all_results[name]['mean_success_rate'] for name in rl_names]
            baseline_success = [all_results[name]['mean_success_rate'] for name in baseline_names]

            plt.boxplot([baseline_success, rl_success], labels=['Baselines', 'RL Agents'])
            plt.title('Success Rate Distribution', fontweight='bold', fontsize=14)
            plt.ylabel('Success Rate (%)')
            plt.grid(True, alpha=0.3)

            # Add statistical test result (simplified)
            if len(rl_success) > 0 and len(baseline_success) > 0:
                rl_mean = np.mean(rl_success)
                baseline_mean = np.mean(baseline_success)
                improvement = ((rl_mean - baseline_mean) / baseline_mean) * 100

                plt.text(0.5, 0.95, f'RL Improvement: {improvement:+.1f}%',
                         transform=plt.gca().transAxes, ha='center', va='top',
                         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),
                         fontweight='bold')

        plt.tight_layout()
        plt.savefig('sagin_comprehensive_comparison.png', dpi=300, bbox_inches='tight')
        print("üìä Comprehensive comparison plots saved as 'sagin_comprehensive_comparison.png'")
        plt.show()

    def save_results_to_csv(self, filename: str = 'sagin_comparison_results.csv'):
        """Save comparison results to CSV file"""
        if not self.comparison_results:
            print("‚ùå No results to save")
            return

        all_results = self.comparison_results['all_results']

        # Prepare data for CSV
        data = []
        for name, results in all_results.items():
            approach_type = 'RL' if ('Hierarchical' in name or 'GRU' in name) else 'Baseline'

            data.append({
                'Approach': name,
                'Type': approach_type,
                'Mean_Success_Rate': results['mean_success_rate'],
                'Std_Success_Rate': results['std_success_rate'],
                'Mean_Cache_Hit_Rate': results['mean_cache_hit_rate'],
                'Std_Cache_Hit_Rate': results['std_cache_hit_rate'],
                'Mean_Energy_Efficiency': results['mean_energy_efficiency'],
                'Std_Energy_Efficiency': results['std_energy_efficiency'],
                'Mean_Dropped_Tasks': results['mean_dropped_tasks'],
                'Std_Dropped_Tasks': results['std_dropped_tasks'],
                'Num_Trials': results['num_trials']
            })

        df = pd.DataFrame(data)
        df = df.sort_values('Mean_Success_Rate', ascending=False)
        df.to_csv(filename, index=False)

        print(f"üíæ Results saved to {filename}")
        print("üìä Top 3 approaches:")
        for i, row in df.head(3).iterrows():
            approach_type = "üß†" if row['Type'] == 'RL' else "üìä"
            print(f"   {i + 1}. {approach_type} {row['Approach']}: {row['Mean_Success_Rate']:.1f}%")

        return df


def main():
    """Main execution function"""
    print("üöÄ COMPREHENSIVE SAGIN COMPARISON: RL vs BASELINES")
    print("=" * 60)

    # Get user configuration
    try:
        grid_input = input("Enter grid size (e.g., '3,3' or press Enter for default): ").strip()
        if grid_input and ',' in grid_input:
            grid_x, grid_y = map(int, grid_input.split(','))
        else:
            grid_x, grid_y = 3, 3

        trials = int(input("Enter number of trials (default=2): ") or "2")

    except (ValueError, KeyboardInterrupt):
        print("Using defaults: 3x3 grid, 2 trials")
        grid_x, grid_y = 3, 3
        trials = 2

    # Initialize comparison framework
    comparison = ComprehensiveComparison(
        grid_size=(grid_x, grid_y),
        num_trials=trials
    )

    # Run comprehensive comparison
    start_time = time.time()

    try:
        results = comparison.run_comprehensive_comparison()

        # Print and plot results
        comparison.print_comparison_summary()
        comparison.plot_comparison_results()

        # Save results
        df = comparison.save_results_to_csv()

        total_time = time.time() - start_time

        # Final summary
        print("\n" + "=" * 80)
        print("üéâ COMPREHENSIVE COMPARISON COMPLETED")
        print("=" * 80)

        print(f"‚è±Ô∏è Total execution time: {total_time:.1f}s")
        print(f"üìä Approaches tested: {len(results['all_results'])}")
        print(f"üî¨ Total trials: {sum(r['num_trials'] for r in results['all_results'].values())}")

        # Key findings
        all_results = results['all_results']
        if all_results:
            best_approach = max(all_results.items(), key=lambda x: x[1]['mean_success_rate'])
            print(f"üèÜ Best overall approach: {best_approach[0]}")
            print(f"   Success rate: {best_approach[1]['mean_success_rate']:.1f}%")

            rl_approaches = {k: v for k, v in all_results.items() if 'Hierarchical' in k or 'GRU' in k}
            if rl_approaches:
                best_rl = max(rl_approaches.items(), key=lambda x: x[1]['mean_success_rate'])
                print(f"üß† Best RL approach: {best_rl[0]}")
                print(f"   Success rate: {best_rl[1]['mean_success_rate']:.1f}%")

        print(f"\nüìà Results visualization saved as 'sagin_comprehensive_comparison.png'")
        print(f"üíæ Detailed results saved as 'sagin_comparison_results.csv'")

        return results

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Comparison interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Comparison failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()