# main_baseline_comparison.py - COMPLETE BASELINE SYSTEM TESTING
import numpy as np
import matplotlib.pyplot as plt
from integrated_baseline_system import IntegratedBaselineSystem, run_baseline_comparison, print_comparison_summary
from sagin_env import SAGINEnv, SystemDownException
import time


def print_header(title):
    """Print formatted header"""
    print(f"\n{'=' * 80}")
    print(f"{title:^80}")
    print(f"{'=' * 80}")


def plot_baseline_comparison(comparison_results):
    """Plot comparison results across different baselines"""

    if not comparison_results:
        print("‚ùå No results to plot")
        return

    # Extract data for plotting
    config_names = []
    success_rates = []
    cache_hit_rates = []
    energy_efficiencies = []
    dropped_tasks = []

    for config_name, results in comparison_results.items():
        if 'avg_success_rate' in results:
            config_names.append(config_name[:20] + '...' if len(config_name) > 20 else config_name)
            success_rates.append(results['avg_success_rate'])
            cache_hit_rates.append(results['avg_cache_hit_rate'])
            energy_efficiencies.append(results['avg_energy_efficiency'])
            dropped_tasks.append(results['dropped_tasks'])

    if not config_names:
        print("‚ùå No valid results to plot")
        return

    # Create comparison plots
    fig = plt.figure(figsize=(16, 10))

    # Success Rate Comparison
    plt.subplot(2, 2, 1)
    bars1 = plt.bar(range(len(config_names)), success_rates,
                    color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(config_names)])
    plt.title('Task Success Rate Comparison', fontweight='bold', fontsize=12)
    plt.ylabel('Success Rate (%)')
    plt.xticks(range(len(config_names)), config_names, rotation=45, ha='right')
    plt.grid(True, alpha=0.3)

    # Add value labels on bars
    for bar, value in zip(bars1, success_rates):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,
                 f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')

    # Cache Hit Rate Comparison
    plt.subplot(2, 2, 2)
    bars2 = plt.bar(range(len(config_names)), cache_hit_rates,
                    color=['lightsteelblue', 'lightpink', 'lightseagreen', 'khaki'][:len(config_names)])
    plt.title('Cache Hit Rate Comparison', fontweight='bold', fontsize=12)
    plt.ylabel('Cache Hit Rate (%)')
    plt.xticks(range(len(config_names)), config_names, rotation=45, ha='right')
    plt.grid(True, alpha=0.3)

    # Add value labels
    for bar, value in zip(bars2, cache_hit_rates):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,
                 f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')

    # Energy Efficiency Comparison
    plt.subplot(2, 2, 3)
    bars3 = plt.bar(range(len(config_names)), energy_efficiencies,
                    color=['cornflowerblue', 'indianred', 'mediumseagreen', 'orange'][:len(config_names)])
    plt.title('Energy Efficiency Comparison', fontweight='bold', fontsize=12)
    plt.ylabel('Tasks per Joule')
    plt.xticks(range(len(config_names)), config_names, rotation=45, ha='right')
    plt.grid(True, alpha=0.3)

    # Add value labels
    for bar, value in zip(bars3, energy_efficiencies):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + max(energy_efficiencies) * 0.01,
                 f'{value:.4f}', ha='center', va='bottom', fontweight='bold')

    # Dropped Tasks Comparison
    plt.subplot(2, 2, 4)
    bars4 = plt.bar(range(len(config_names)), dropped_tasks,
                    color=['lightblue', 'lightsalmon', 'palegreen', 'moccasin'][:len(config_names)])
    plt.title('Dropped Tasks Comparison', fontweight='bold', fontsize=12)
    plt.ylabel('Number of Dropped Tasks')
    plt.xticks(range(len(config_names)), config_names, rotation=45, ha='right')
    plt.grid(True, alpha=0.3)

    # Add value labels
    for bar, value in zip(bars4, dropped_tasks):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + max(dropped_tasks) * 0.01,
                 f'{value}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.savefig('baseline_comparison_results.png', dpi=300, bbox_inches='tight')
    print(f"üìä Comparison plots saved as 'baseline_comparison_results.png'")
    plt.show()


def run_single_baseline_demo():
    """Run a single baseline configuration as a demonstration"""

    print_header("SINGLE BASELINE DEMONSTRATION")

    print("üîß Initializing system with best baseline configuration...")

    # Create environment
    env = SAGINEnv(
        X=3, Y=3, duration=300, cache_size=40, compute_power_uav=25,
        compute_power_sat=200, energy=80000, max_queue=15, num_sats=2,
        num_iot_per_region=20, max_active_iot=10, ofdm_slots=6
    )

    # Initialize baseline system (using recommended configuration)
    baseline_system = IntegratedBaselineSystem(
        aggregation_type='greedy',
        caching_type='greedy',
        offloading_type='content_aware',
        ofdm_type='priority_based'
    )
    baseline_system.initialize_baselines(env)

    # Run demonstration
    timesteps = 15
    performance_history = []

    print(f"\nüöÄ Running {timesteps}-timestep demonstration...")

    try:
        for timestep in range(timesteps):
            print(f"\n--- Timestep {timestep + 1}/{timesteps} ---")

            # Execute baseline step
            baseline_system.execute_baseline_step(env)

            # Collect performance
            performance = env.get_performance_summary()
            performance_history.append(performance)

            # Print timestep results
            print(f"‚úÖ Success Rate: {performance['overall_success_rate']:.1f}%")
            print(f"üìä Cache Hit Rate: {performance['cache_hit_rate']:.1f}%")
            print(f"‚ö° Energy Efficiency: {performance['energy_efficiency']:.4f}")
            print(f"‚ùå Dropped Tasks: {performance['dropped_tasks']}")

            # Energy check
            min_energy = min(uav.energy for uav in env.uavs.values())
            avg_energy = np.mean([uav.energy for uav in env.uavs.values()])
            print(f"üîã Energy: Min={min_energy:.0f}J, Avg={avg_energy:.0f}J")

            if min_energy <= 0:
                print("‚ö†Ô∏è  Energy depleted!")
                break

    except SystemDownException as e:
        print(f"üõë System shutdown: {e}")

    # Print final summary
    env.print_final_summary()

    return performance_history


def print_baseline_descriptions():
    """Print descriptions of all implemented baselines"""

    print_header("IMPLEMENTED BASELINE APPROACHES")

    print("üî¨ The following baseline approaches are implemented for comparison:")
    print("")

    print("1Ô∏è‚É£  IoT DATA AGGREGATION BASELINES:")
    print("   üìä Greedy: Popularity-to-size ratio, greedy selection within TDMA constraints")
    print("   üß† GRU Bandit: GRU temporal context + immediate reward (no credit assignment)")
    print("   üé≤ Random: Random device selection within TDMA constraints")
    print("")

    print("2Ô∏è‚É£  CONTENT CACHING BASELINES:")
    print("   üìä Greedy: Rank by usefulness/size ratio, fill cache until full")
    print("   üß† Stateless PPO: Neural network based on item metadata only")
    print("")

    print("3Ô∏è‚É£  TASK OFFLOADING BASELINES:")
    print("   üé≤ Random Split: Uniform random distribution across neighbors+satellite")
    print("   üéØ Content Aware: Greedy based on content availability and queue status")
    print("   ‚öñÔ∏è  Load Balanced: Minimize queue imbalance across execution locations")
    print("")

    print("4Ô∏è‚É£  OFDM SLOT ALLOCATION BASELINES:")
    print("   üìä Backlog Greedy: Select UAVs with largest aggregated data")
    print("   üîÑ Round Robin: Cyclic assignment irrespective of load")
    print("   üé≤ Random: Uniform random UAV selection")
    print("   üéØ Priority Based: Combine data backlog + queue urgency")
    print("   ‚öñÔ∏è  Fairness Aware: Historical allocation balance")


def main():
    """Main execution function"""

    print("üöÄ SAGIN BASELINE APPROACHES COMPARISON")
    print("=" * 80)

    # Print baseline descriptions
    print_baseline_descriptions()

    # Get user choice
    print_header("EXECUTION OPTIONS")
    print("1. Run single baseline demonstration (quick)")
    print("2. Run comprehensive baseline comparison (thorough)")
    print("3. Run both")

    try:
        choice = input("\nEnter your choice (1/2/3, default=1): ").strip() or "1"
    except:
        choice = "1"

    results = {}

    if choice in ["1", "3"]:
        # Run single baseline demo
        performance_history = run_single_baseline_demo()
        results['demo'] = performance_history

    if choice in ["2", "3"]:
        # Run comprehensive comparison
        print_header("COMPREHENSIVE BASELINE COMPARISON")

        # Define configurations to compare
        baseline_configs = [
            {
                'aggregation': 'greedy',
                'caching': 'greedy',
                'offloading': 'content_aware',
                'ofdm': 'priority_based'
            },
            {
                'aggregation': 'random',
                'caching': 'greedy',
                'offloading': 'random_split',
                'ofdm': 'backlog_greedy'
            },
            {
                'aggregation': 'greedy',
                'caching': 'greedy',
                'offloading': 'load_balanced',
                'ofdm': 'round_robin'
            },
            {
                'aggregation': 'greedy',
                'caching': 'greedy',
                'offloading': 'content_aware',
                'ofdm': 'fairness_aware'
            }
        ]

        print(f"üî¨ Comparing {len(baseline_configs)} different configurations...")

        start_time = time.time()
        comparison_results = run_baseline_comparison(
            baseline_configs, episodes=1, timesteps_per_episode=15
        )
        total_time = time.time() - start_time

        # Print and plot results
        print_comparison_summary(comparison_results)
        plot_baseline_comparison(comparison_results)

        results['comparison'] = comparison_results
        print(f"\n‚è±Ô∏è  Total comparison time: {total_time:.1f}s")

    # Final summary
    print_header("BASELINE SYSTEM VALIDATION COMPLETE")

    print("üéâ Key Achievements:")
    print("   ‚úÖ Complete system model implementation (Paper Equations 1-29)")
    print("   ‚úÖ All four baseline approaches implemented and tested")
    print("   ‚úÖ Proper C-band/Ka-band communication with TDMA/OFDMA")
    print("   ‚úÖ Spatiotemporal Zipf IoT activation2 patterns")
    print("   ‚úÖ Energy-aware system with realistic consumption models")
    print("   ‚úÖ Content-aware task offloading with baseline strategies")
    print("   ‚úÖ OFDM constraint enforcement and slot allocation")
    print("   ‚úÖ Comprehensive performance metrics and visualization")
    print("")
    print("üöÄ System Status:")
    print("   üìä Baseline approaches validated and ready for comparison")
    print("   üî¨ System model properly aligned with research paper")
    print("   ‚ö° Realistic energy consumption and system longevity")
    print("   üìà Performance metrics collection and analysis working")
    print("")
    print("üéØ Next Steps:")
    print("   1. Implement hierarchical RL agents (GRU-PPO + MAPPO)")
    print("   2. Train RL agents using this baseline system as environment")
    print("   3. Compare RL agent performance against these baselines")
    print("   4. Generate final research results and analysis")

    return results


if __name__ == "__main__":
    try:
        results = main()
    except KeyboardInterrupt:
        print(f"\n\n‚èπÔ∏è  Baseline testing interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Error during baseline testing: {e}")
        import traceback

        traceback.print_exc()