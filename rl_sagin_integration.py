# rl_sagin_integration.py - Integration of RL Agents with SAGIN Environment
import numpy as np
import torch
import time
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, deque
import matplotlib.pyplot as plt

# Import the existing SAGIN environment and RL agents
from sagin_env import SAGINEnv, SystemDownException
from rl_formulation_sagin import HierarchicalSAGINAgent


class RLSAGINEnvironmentWrapper:
    """
    Wrapper that adapts the existing SAGIN environment for RL agent training
    Handles state extraction, action application, and reward computation
    """

    def __init__(self, sagin_env: SAGINEnv):
        self.env = sagin_env
        self.timestep = 0
        self.episode_history = []
        self.performance_metrics = {
            'success_rates': [],
            'cache_hit_rates': [],
            'energy_efficiency': [],
            'dropped_tasks': [],
            'total_rewards': []
        }

    def reset(self):
        """Reset environment for new episode"""
        # Reset SAGIN environment state
        self.timestep = 0

        # Reset UAV states
        for uav in self.env.uavs.values():
            uav.cache_storage = {}
            uav.cache_used_mb = 0.0
            uav.aggregated_content = {}
            uav.queue = []
            uav.next_available_time = 0
            uav.energy_used_this_slot = 0.0
            # Don't reset energy completely - use degraded starting energy for realism
            uav.energy = max(uav.energy, uav.max_energy * 0.7)

        # Reset satellites
        for sat in self.env.sats:
            sat.task_queue = []
            sat.local_storage = {}
            sat.storage_used_mb = 0.0

        # Reset global state
        self.env.global_satellite_content_pool = {}
        self.env.subchannel_assignments = {}
        self.env.connected_uavs = set()
        self.env.g_timestep = -1

        return self.get_initial_state()

    def get_initial_state(self) -> Dict[str, Any]:
        """Get initial state for RL agent"""
        return {
            'uav_states': self.get_uav_states(),
            'active_devices': {},
            'device_contents': {},
            'task_bursts': {},
            'candidate_content': {},
            'uav_positions': self.get_uav_positions()
        }

    def get_uav_states(self) -> Dict[Tuple[int, int], Dict[str, Any]]:
        """Extract UAV states for RL agent"""
        uav_states = {}

        for (x, y), uav in self.env.uavs.items():
            # Calculate current state metrics
            energy_ratio = uav.energy / uav.max_energy
            queue_ratio = len(uav.queue) / uav.max_queue
            cache_hit_rate = uav.cache_hits / max(uav.total_tasks, 1)

            # Get aggregated data size
            aggregated_data = sum(content.get('size', 0) for content in uav.aggregated_content.values())

            # Get Zipf parameter from region
            region = self.env.iot_regions.get((x, y))
            zipf_param = getattr(region, 'current_zipf_param', 1.5)

            uav_states[(x, y)] = {
                'energy_ratio': energy_ratio,
                'queue_ratio': queue_ratio,
                'cache_hit_rate': cache_hit_rate,
                'aggregated_data': aggregated_data,
                'zipf_param': zipf_param,
                'cache_capacity': uav.cache_capacity_mb,
                'cache_items': list(uav.cache_storage.keys()),
                'active_devices': [],
                'content_generation': {}
            }

        return uav_states

    def get_uav_positions(self) -> Dict[Tuple[int, int], Tuple[float, float, float]]:
        """Get UAV 3D positions"""
        return {coord: uav.uav_pos for coord, uav in self.env.uavs.items()}

    def get_active_devices(self) -> Dict[Tuple[int, int], List[int]]:
        """Get active IoT devices per region"""
        active_devices = {}

        for (x, y), region in self.env.iot_regions.items():
            # Sample active devices using the region's spatiotemporal model
            active_device_ids = region.sample_active_devices()
            active_devices[(x, y)] = active_device_ids

        return active_devices

    def get_device_contents(self) -> Dict[Tuple[int, int], Dict[int, Dict]]:
        """Get content generated by active devices"""
        device_contents = {}

        for (x, y), region in self.env.iot_regions.items():
            # Get active devices for this region
            active_devices = region.sample_active_devices()

            if active_devices:
                # Generate content from active devices
                content_list = region.generate_content(active_devices, self.timestep, (x, y))

                # Convert to device_id -> content mapping
                region_contents = {}
                for content in content_list:
                    device_id = content.get('device_id')
                    if device_id is not None:
                        region_contents[device_id] = content

                device_contents[(x, y)] = region_contents
            else:
                device_contents[(x, y)] = {}

        return device_contents

    def get_task_bursts(self) -> Dict[Tuple[int, int], List[Dict]]:
        """Get task bursts for each UAV"""
        task_bursts = {}

        for (x, y), uav in self.env.uavs.items():
            # Generate tasks for this UAV
            tasks = uav.generate_tasks(self.env.X, self.env.Y, self.timestep)
            task_bursts[(x, y)] = tasks

        return task_bursts

    def get_candidate_content(self) -> Dict[Tuple[int, int], List[Dict]]:
        """Get candidate content for caching decisions"""
        candidate_content = {}

        for (x, y), uav in self.env.uavs.items():
            candidates = []

            # Add existing cache content
            for cid, content in uav.cache_storage.items():
                content_copy = content.copy()
                content_copy['origin'] = 1  # From cache
                candidates.append(content_copy)

            # Add aggregated content
            for cid, content in uav.aggregated_content.items():
                content_copy = content.copy()
                content_copy['origin'] = 0  # From aggregation
                candidates.append(content_copy)

            # Add satellite content if connected
            is_connected = (x, y) in self.env.connected_uavs
            if is_connected:
                for cid, content in self.env.global_satellite_content_pool.items():
                    if cid not in uav.cache_storage and cid not in uav.aggregated_content:
                        content_copy = content.copy()
                        content_copy['origin'] = 2  # From satellite
                        candidates.append(content_copy)

            candidate_content[(x, y)] = candidates

        return candidate_content

    def apply_iot_aggregation(self, selected_devices: Dict[Tuple[int, int], List[int]]):
        """Apply IoT aggregation decisions to environment"""
        for (x, y), device_ids in selected_devices.items():
            if (x, y) not in self.env.uavs:
                continue

            uav = self.env.uavs[(x, y)]
            region = self.env.iot_regions[(x, y)]

            # Generate content for selected devices only
            if device_ids:
                content_list = region.generate_content(device_ids, self.timestep, (x, y))
                content_dict = {tuple(c['id']): c for c in content_list}

                # Calculate interference
                interfering_regions = [coord for coord in self.env.uavs.keys() if coord != (x, y)]

                # Aggregate content using existing UAV method
                uav.aggregate_content(content_dict, interfering_regions)
            else:
                uav.aggregated_content = {}

    def apply_ofdm_allocation(self, slot_allocation: Dict[Tuple[int, int], bool]):
        """Apply OFDM slot allocation decisions"""
        # Reset allocations
        self.env.subchannel_assignments = {coord: {} for coord in self.env.uavs.keys()}
        self.env.connected_uavs = set()

        # Apply new allocations
        allocated_slots = 0
        sat_assignments = defaultdict(list)

        # Sort by priority if needed (for now, use the provided allocation)
        for (x, y), is_allocated in slot_allocation.items():
            if is_allocated and allocated_slots < self.env.ofdm_slots:
                # Find satellite with least assignments for load balancing
                best_sat = min(self.env.sats, key=lambda s: len(sat_assignments[s.sat_id]))

                # Assign subchannel
                self.env.subchannel_assignments[(x, y)][best_sat.sat_id] = True
                sat_assignments[best_sat.sat_id].append((x, y))
                self.env.connected_uavs.add((x, y))
                allocated_slots += 1

    def apply_offloading_decisions(self, offloading_decisions: Dict[Tuple[int, int], torch.Tensor]):
        """Apply task offloading decisions"""
        for (x, y), allocation in offloading_decisions.items():
            if (x, y) not in self.env.uavs:
                continue

            uav = self.env.uavs[(x, y)]

            # Generate tasks if not already done
            tasks = uav.generate_tasks(self.env.X, self.env.Y, self.timestep)

            if not tasks or allocation is None:
                continue

            # Convert allocation tensor to decisions
            allocation_list = allocation.tolist() if isinstance(allocation, torch.Tensor) else allocation

            # Apply allocation (simplified - distribute tasks based on allocation)
            task_idx = 0
            categories = ['local', 'neighbor_0', 'neighbor_1', 'neighbor_2', 'neighbor_3', 'satellite', 'drop']

            for cat_idx, count in enumerate(allocation_list):
                if cat_idx >= len(categories):
                    break

                for _ in range(int(count)):
                    if task_idx >= len(tasks):
                        break

                    task = tasks[task_idx]
                    category = categories[cat_idx]

                    # Apply the offloading decision
                    self._execute_task_offloading(task, (x, y), category)
                    task_idx += 1

    def _execute_task_offloading(self, task: Dict, uav_coord: Tuple[int, int], target: str):
        """Execute individual task offloading decision"""
        cid = tuple(task['content_id'])

        if target == 'local':
            uav = self.env.uavs[uav_coord]
            if cid in uav.cache_storage or cid in uav.aggregated_content:
                uav.receive_task(task, from_coord=uav_coord)

        elif target.startswith('neighbor_'):
            # Find valid neighbors (4-connected)
            x, y = uav_coord
            neighbor_offsets = [(-1, 0), (1, 0), (0, -1), (0, 1)]
            valid_neighbors = []

            for dx, dy in neighbor_offsets:
                nx, ny = x + dx, y + dy
                if (nx, ny) in self.env.uavs:
                    neighbor = self.env.uavs[(nx, ny)]
                    if (cid in neighbor.cache_storage or cid in neighbor.aggregated_content) and \
                            len(neighbor.queue) < neighbor.max_queue:
                        valid_neighbors.append((nx, ny))

            if valid_neighbors:
                # Select first available neighbor (could be improved)
                neighbor_coord = valid_neighbors[0]
                neighbor = self.env.uavs[neighbor_coord]
                neighbor.receive_task(task, from_coord=uav_coord)

        elif target == 'satellite':
            # Find assigned satellite for this UAV
            uav_assignments = self.env.subchannel_assignments.get(uav_coord, {})
            for sat in self.env.sats:
                if uav_assignments.get(sat.sat_id, False) and cid in self.env.global_satellite_content_pool:
                    sat.receive_task(task, from_coord=self.env.uavs[uav_coord].uav_pos)
                    break

        # 'drop' case - do nothing, task is dropped

    def apply_caching_decisions(self, caching_decisions: Dict[Tuple[int, int], List[Dict]]):
        """Apply content caching decisions"""
        for (x, y), selected_content in caching_decisions.items():
            if (x, y) not in self.env.uavs:
                continue

            uav = self.env.uavs[(x, y)]

            # Update cache with selected content
            new_cache = {}
            total_size = 0.0

            for content in selected_content:
                cid = tuple(content.get('id', (x, y, 0)))
                size = content.get('size', 1.0)

                if total_size + size <= uav.cache_capacity_mb:
                    new_cache[cid] = content
                    total_size += size

            uav.cache_storage = new_cache
            uav.cache_used_mb = total_size

    def step(self) -> Dict[str, Dict[str, float]]:
        """Execute one environment step and return rewards"""
        self.timestep += 1

        # Update satellite coverage
        for sat in self.env.sats:
            sat.update_coverage(self.timestep)

        # Upload to satellites
        self.env.upload_to_satellites()

        # Synchronize satellites
        self.env.sync_satellites()

        # Execute tasks
        self.env.execute_all_tasks()

        # Update caches (already handled by RL decisions)
        self.env.evict_expired_content()

        # Compute rewards
        rewards = self._compute_rewards()

        # Update performance metrics
        performance = self.env.get_performance_summary()
        self.performance_metrics['success_rates'].append(performance['overall_success_rate'])
        self.performance_metrics['cache_hit_rates'].append(performance['cache_hit_rate'])
        self.performance_metrics['energy_efficiency'].append(performance['energy_efficiency'])
        self.performance_metrics['dropped_tasks'].append(performance['dropped_tasks'])

        return rewards

    def _compute_rewards(self) -> Dict[str, Dict[str, float]]:
        """Compute rewards for RL agents"""
        rewards = {
            'iot_aggregation': {},
            'caching_offloading': {},
            'ofdm_allocation': 0.0
        }

        # IoT Aggregation rewards - based on usefulness of selected devices
        for (x, y), uav in self.env.uavs.items():
            agent_key = f"{x}_{y}"

            # Reward based on content utility
            aggregation_reward = 0.0
            for cid, content in uav.aggregated_content.items():
                # Simple utility: inverse of size (smaller content = higher utility)
                utility = 1.0 / max(content.get('size', 1.0), 0.1)
                aggregation_reward += utility

            rewards['iot_aggregation'][agent_key] = aggregation_reward

        # Caching and Offloading rewards
        for (x, y), uav in self.env.uavs.items():
            agent_key = f"{x}_{y}"

            # Task completion reward
            completed_tasks = sum(1 for task in getattr(uav, 'completed_tasks_this_step', []))

            # Cache hit reward
            cache_hit_reward = uav.cache_hits * 0.1

            # Energy penalty
            energy_penalty = uav.energy_used_this_slot / uav.max_energy * 0.05

            # Queue penalty
            queue_penalty = len(uav.queue) / uav.max_queue * 0.02

            total_reward = completed_tasks + cache_hit_reward - energy_penalty - queue_penalty
            rewards['caching_offloading'][agent_key] = total_reward

        # OFDM allocation reward - global throughput
        total_uploaded = sum(
            sum(content.get('size', 0) for content in uav.aggregated_content.values())
            for (x, y) in self.env.connected_uavs
            for uav in [self.env.uavs[(x, y)]]
        )

        # Energy cost for transmission
        transmission_energy = len(self.env.connected_uavs) * 0.5  # Simplified

        rewards['ofdm_allocation'] = total_uploaded - transmission_energy * 0.01

        return rewards

    def is_done(self) -> bool:
        """Check if episode should terminate"""
        # Episode ends if any UAV runs out of energy
        for uav in self.env.uavs.values():
            if uav.energy <= 0:
                return True

        # Or after maximum timesteps
        return self.timestep >= 50

    def get_performance_metrics(self) -> Dict[str, List[float]]:
        """Get performance metrics for analysis"""
        return self.performance_metrics.copy()


class RLSAGINTrainer:
    """
    Training orchestrator for hierarchical RL agents in SAGIN environment
    """

    def __init__(self, grid_size: Tuple[int, int] = (3, 3),
                 cache_size: int = 40, num_episodes: int = 1000):

        # Initialize SAGIN environment
        self.sagin_env = SAGINEnv(
            X=grid_size[0], Y=grid_size[1], duration=300, cache_size=cache_size,
            compute_power_uav=25, compute_power_sat=200, energy=80000,
            max_queue=15, num_sats=2, num_iot_per_region=20,
            max_active_iot=10, ofdm_slots=6
        )

        # Initialize RL environment wrapper
        self.env_wrapper = RLSAGINEnvironmentWrapper(self.sagin_env)

        # Initialize hierarchical RL agent
        self.rl_agent = HierarchicalSAGINAgent(
            grid_size=grid_size, max_devices=20,
            max_content_items=50, max_neighbors=4
        )

        self.num_episodes = num_episodes
        self.training_history = {
            'episode_rewards': [],
            'success_rates': [],
            'cache_hit_rates': [],
            'energy_efficiency': [],
            'training_time': []
        }

    def train(self, save_interval: int = 100, plot_results: bool = True) -> Dict[str, List[float]]:
        """
        Train the hierarchical RL agent

        Args:
            save_interval: Save model every N episodes
            plot_results: Whether to plot training results

        Returns:
            training_history: Dictionary of training metrics
        """
        print(f"üöÄ Starting RL Training: {self.num_episodes} episodes")
        print(f"üìä Environment: {self.sagin_env.X}x{self.sagin_env.Y} grid, {len(self.sagin_env.sats)} satellites")
        print(f"üß† Agent: Hierarchical GRU-PPO + MAPPO")
        print("=" * 80)

        start_time = time.time()

        for episode in range(self.num_episodes):
            episode_start = time.time()

            # Reset environment and agent
            state = self.env_wrapper.reset()
            self.rl_agent.reset_temporal_states()

            episode_reward = 0.0
            step_rewards = {
                'iot_aggregation': defaultdict(list),
                'caching_offloading': defaultdict(list),
                'ofdm_allocation': []
            }

            try:
                # Run episode
                for timestep in range(50):
                    # Execute hierarchical RL decisions

                    # 1. IoT Aggregation
                    selected_devices = self.rl_agent.step_iot_aggregation(
                        state['uav_states'],
                        self.env_wrapper.get_active_devices(),
                        self.env_wrapper.get_device_contents()
                    )

                    # 2. OFDM Slot Allocation
                    slot_allocation = self.rl_agent.step_ofdm_allocation(
                        state['uav_states'], max_slots=6
                    )

                    # 3. Caching and Offloading
                    offloading_decisions, caching_decisions = self.rl_agent.step_caching_offloading(
                        state['uav_states'],
                        self.env_wrapper.get_task_bursts(),
                        self.env_wrapper.get_candidate_content(),
                        state['uav_positions']
                    )

                    # Apply decisions to environment
                    self.env_wrapper.apply_iot_aggregation(selected_devices)
                    self.env_wrapper.apply_ofdm_allocation(slot_allocation)
                    self.env_wrapper.apply_offloading_decisions(offloading_decisions)
                    self.env_wrapper.apply_caching_decisions(caching_decisions)

                    # Environment step and reward computation
                    rewards = self.env_wrapper.step()

                    # Accumulate rewards
                    for agent_key, reward in rewards.get('iot_aggregation', {}).items():
                        step_rewards['iot_aggregation'][agent_key].append(reward)

                    for agent_key, reward in rewards.get('caching_offloading', {}).items():
                        step_rewards['caching_offloading'][agent_key].append(reward)

                    step_rewards['ofdm_allocation'].append(rewards.get('ofdm_allocation', 0.0))

                    # Calculate episode reward
                    timestep_reward = (
                            sum(rewards.get('iot_aggregation', {}).values()) +
                            sum(rewards.get('caching_offloading', {}).values()) +
                            rewards.get('ofdm_allocation', 0.0)
                    )
                    episode_reward += timestep_reward

                    # Update state
                    state = {
                        'uav_states': self.env_wrapper.get_uav_states(),
                        'uav_positions': state['uav_positions']
                    }

                    # Check for early termination
                    if self.env_wrapper.is_done():
                        break

            except SystemDownException as e:
                print(f"Episode {episode}: System shutdown - {e}")

            # Update RL agents
            self.rl_agent.update_agents(step_rewards)

            # Record metrics
            episode_duration = time.time() - episode_start
            performance_metrics = self.env_wrapper.get_performance_metrics()

            self.training_history['episode_rewards'].append(episode_reward)
            self.training_history['training_time'].append(episode_duration)

            if performance_metrics['success_rates']:
                self.training_history['success_rates'].append(performance_metrics['success_rates'][-1])
                self.training_history['cache_hit_rates'].append(performance_metrics['cache_hit_rates'][-1])
                self.training_history['energy_efficiency'].append(performance_metrics['energy_efficiency'][-1])

            # Progress reporting
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(self.training_history['episode_rewards'][-10:])
                avg_success = np.mean(self.training_history['success_rates'][-10:]) if self.training_history[
                    'success_rates'] else 0
                avg_cache = np.mean(self.training_history['cache_hit_rates'][-10:]) if self.training_history[
                    'cache_hit_rates'] else 0

                print(f"Episode {episode + 1:4d} | "
                      f"Reward: {avg_reward:7.2f} | "
                      f"Success: {avg_success:5.1f}% | "
                      f"Cache: {avg_cache:5.1f}% | "
                      f"Time: {episode_duration:.2f}s")

            # Save checkpoint
            if (episode + 1) % save_interval == 0:
                self._save_checkpoint(episode + 1)

        total_training_time = time.time() - start_time
        print(f"\n‚úÖ Training completed in {total_training_time:.1f}s")
        print(f"üìà Average reward: {np.mean(self.training_history['episode_rewards']):.2f}")

        if plot_results:
            self.plot_training_results()

        return self.training_history

    def _save_checkpoint(self, episode: int):
        """Save model checkpoint"""
        checkpoint = {
            'episode': episode,
            'agent_state_dict': self.rl_agent.state_dict(),
            'training_history': self.training_history
        }
        torch.save(checkpoint, f'sagin_rl_checkpoint_ep{episode}.pt')
        print(f"üíæ Checkpoint saved at episode {episode}")

    def plot_training_results(self):
        """Plot training progress and performance metrics"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Episode rewards
        axes[0, 0].plot(self.training_history['episode_rewards'])
        axes[0, 0].set_title('Episode Rewards', fontweight='bold')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Total Reward')
        axes[0, 0].grid(True, alpha=0.3)

        # Success rates
        if self.training_history['success_rates']:
            axes[0, 1].plot(self.training_history['success_rates'], color='green')
            axes[0, 1].set_title('Task Success Rate', fontweight='bold')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('Success Rate (%)')
            axes[0, 1].grid(True, alpha=0.3)

        # Cache hit rates
        if self.training_history['cache_hit_rates']:
            axes[1, 0].plot(self.training_history['cache_hit_rates'], color='orange')
            axes[1, 0].set_title('Cache Hit Rate', fontweight='bold')
            axes[1, 0].set_xlabel('Episode')
            axes[1, 0].set_ylabel('Hit Rate (%)')
            axes[1, 0].grid(True, alpha=0.3)

        # Training time per episode
        if self.training_history['training_time']:
            axes[1, 1].plot(self.training_history['training_time'], color='red')
            axes[1, 1].set_title('Training Time per Episode', fontweight='bold')
            axes[1, 1].set_xlabel('Episode')
            axes[1, 1].set_ylabel('Time (s)')
            axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('rl_sagin_training_results.png', dpi=300, bbox_inches='tight')
        print("üìä Training results saved as 'rl_sagin_training_results.png'")
        plt.show()

    def evaluate(self, num_episodes: int = 10) -> Dict[str, float]:
        """
        Evaluate trained agent performance

        Args:
            num_episodes: Number of evaluation episodes

        Returns:
            evaluation_metrics: Dictionary of performance metrics
        """
        print(f"üìä Evaluating trained agent over {num_episodes} episodes...")

        eval_metrics = {
            'avg_reward': [],
            'avg_success_rate': [],
            'avg_cache_hit_rate': [],
            'avg_energy_efficiency': [],
            'avg_dropped_tasks': []
        }

        self.rl_agent.eval()  # Set to evaluation mode

        with torch.no_grad():
            for eval_ep in range(num_episodes):
                # Reset environment
                state = self.env_wrapper.reset()
                self.rl_agent.reset_temporal_states()

                episode_reward = 0.0

                try:
                    for timestep in range(50):
                        # Execute RL decisions (without exploration)
                        selected_devices = self.rl_agent.step_iot_aggregation(
                            state['uav_states'],
                            self.env_wrapper.get_active_devices(),
                            self.env_wrapper.get_device_contents()
                        )

                        slot_allocation = self.rl_agent.step_ofdm_allocation(state['uav_states'])

                        offloading_decisions, caching_decisions = self.rl_agent.step_caching_offloading(
                            state['uav_states'],
                            self.env_wrapper.get_task_bursts(),
                            self.env_wrapper.get_candidate_content(),
                            state['uav_positions']
                        )

                        # Apply decisions
                        self.env_wrapper.apply_iot_aggregation(selected_devices)
                        self.env_wrapper.apply_ofdm_allocation(slot_allocation)
                        self.env_wrapper.apply_offloading_decisions(offloading_decisions)
                        self.env_wrapper.apply_caching_decisions(caching_decisions)

                        # Environment step
                        rewards = self.env_wrapper.step()

                        episode_reward += (
                                sum(rewards.get('iot_aggregation', {}).values()) +
                                sum(rewards.get('caching_offloading', {}).values()) +
                                rewards.get('ofdm_allocation', 0.0)
                        )

                        # Update state
                        state = {
                            'uav_states': self.env_wrapper.get_uav_states(),
                            'uav_positions': state['uav_positions']
                        }

                        if self.env_wrapper.is_done():
                            break

                except SystemDownException:
                    pass

                # Collect metrics
                performance = self.sagin_env.get_performance_summary()
                eval_metrics['avg_reward'].append(episode_reward)
                eval_metrics['avg_success_rate'].append(performance['overall_success_rate'])
                eval_metrics['avg_cache_hit_rate'].append(performance['cache_hit_rate'])
                eval_metrics['avg_energy_efficiency'].append(performance['energy_efficiency'])
                eval_metrics['avg_dropped_tasks'].append(performance['dropped_tasks'])

        self.rl_agent.train()  # Set back to training mode

        # Calculate averages
        final_metrics = {key: np.mean(values) for key, values in eval_metrics.items()}

        print("üìà Evaluation Results:")
        print(f"   Average Reward: {final_metrics['avg_reward']:.2f}")
        print(f"   Average Success Rate: {final_metrics['avg_success_rate']:.1f}%")
        print(f"   Average Cache Hit Rate: {final_metrics['avg_cache_hit_rate']:.1f}%")
        print(f"   Average Energy Efficiency: {final_metrics['avg_energy_efficiency']:.4f}")
        print(f"   Average Dropped Tasks: {final_metrics['avg_dropped_tasks']:.1f}")

        return final_metrics


# Example usage and training execution
if __name__ == "__main__":
    print("ü§ñ SAGIN Hierarchical RL Training System")
    print("=" * 60)

    # Initialize trainer
    trainer = RLSAGINTrainer(
        grid_size=(3, 3),
        cache_size=40,
        num_episodes=500  # Reduced for initial testing
    )

    print(f"üîß System initialized:")
    print(f"   Grid size: {trainer.sagin_env.X}x{trainer.sagin_env.Y}")
    print(f"   UAVs: {len(trainer.sagin_env.uavs)}")
    print(f"   Satellites: {len(trainer.sagin_env.sats)}")
    print(f"   RL Agent parameters: {sum(p.numel() for p in trainer.rl_agent.parameters()):,}")

    # Start training
    try:
        training_history = trainer.train(save_interval=50, plot_results=True)

        # Evaluate trained agent
        eval_results = trainer.evaluate(num_episodes=10)

        print("\nüéâ Training and evaluation completed successfully!")
        print(f"üìä Final performance: {eval_results['avg_success_rate']:.1f}% success rate")

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Training interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Training error: {e}")
        import traceback

        traceback.print_exc()